<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"
          xmlns:pref="http://www.w3.org/2002/Math/preference"
      pref:renderer="css">

<head>
<script language="JavaScript1.4" type="text/javascript"><!--
        pageModDate = "Saturday 1 March 2008 10:10 PST";
        // copyright 1997-2008 by P.B. Stark, statistics.berkeley.edu/~stark.
    // All rights reserved.
// -->
</script>


<script language="JavaScript1.4" type="text/javascript" src="../../../Java/irGrade.js">
</script>
<script language="JavaScript1.4" type="text/javascript"><!--
    var cNum = "240.2";
    writeChapterHead('SeEd',cNum,'Statistics 240 Notes, part 2',false,'../../../SticiGui/',false);
// -->
</script>
</head>

<body onload="setApplets()" onunload="killApplets()">
<script language="JavaScript1.4" type="text/javascript"><!--
//    writeChapterNav('../..');
    writeChapterTitle();
// -->
</script>

<noscript>
    You need a browser that supports JavaScript and Java to use this site,
    and you must enable JavaScript and Java in your browser.
</noscript>

<form method="post">

<p>
    Topics for (presumed) review:
</p>

<ul>
    <li>
        Bernoulli trials, binomial distribution, hypergeometric distribution.

    </li>
    <li>
        Hypothesis testing: null and alternative hypotheses, significance level,
        power, <em>P</em>-values.
    </li>
</ul>

<p>
    Reference: <a href="../../../SticiGui/index.htm">SticiGui</a>.
</p>

<h2>Some discrete distributions</h2>

<p>
    Bernoulli trial: two possible outcomes, success and failure.
    The probability of success is <em>p</em>; the
    probability of failure is 1&minus;<em>p</em>.
    Recall the definition of <em>independence</em> for a collection of events and for a collection
    of random variables; the heuristic is that they are uninformative with respect to each other.
    Events <em>A</em> and <em>B</em> are independent if P(<em>A</em><em>B</em>) =
    P(<em>A</em>)&times;P(<em>B</em>).
    A collection of events is independent if the probability of the intersection
    of every subcollection is equal to the product of the probabilities of the members of that
    subcollection.
    Two random variables are independent if every event <em>determined by</em> the first
    random variable is independent of every event determined by the second.
</p>

<p>
    Consider a sequence of <em>n</em> independent Bernoulli trials with the same probability
    <em>p</em> of success in each trial.
    Probability distribution of the total number <em>X</em> of successes is <em>binomial</em>:
</p>

<p align="center">
    P(<em>X</em>=<em>x</em>) =
    <sub><em>n</em></sub>C<sub><em>x</em></sub>
    <em>p</em><sup><em>x</em></sup>
    (1&minus;<em>p</em>)<sup><em>n</em>&minus;<em>x</em></sup>.
</p>

<p>
    A <em>simple random sample of size</em> <em>n</em> from a finite population of <em>N</em>
    things is a random sample drawn without replacement in such a way that each of the
    <sub><em>N</em></sub>C<sub><em>n</em></sub> subsets of size <em>n</em> from
    the population is equally likely to be the sample.
    Consider drawing a simple random sample from a population of <em>N</em> objects
    of which <em>G</em> are good and <em>N</em>&minus;<em>G</em> are bad.
    The probability distribution of the number <em>X</em> of good objects in the sample is
    <em>hypergeometric</em>:
</p>

<p align="center">
    P(<em>X</em>=<em>x</em>) = <sub><em>G</em></sub>C<sub><em>x</em></sub> &times;
        <sub><em>N</em>&minus;<em>G</em></sub>C<sub><em>n</em>&minus;<em>x</em></sub>/<sub><em>N</em></sub>C<sub><em>n</em></sub>,
</p>

<p>
    for max(0, <em>n</em>&minus;(<em>N</em>&minus;<em>G</em>)) &le;
    <em>x</em> &le; min(<em>n</em>, <em>G</em>).
</p>

<h2>Review of Hypothesis Testing</h2>

<p>
    Null and alternative hypotheses; type I and type II errors. Neyman-Pearson paradigm.
    Significance level, power, <em>P</em>-values.
</p>

<p>
    Null and alternative hypotheses are competing theories about the world.
    The labeling sometimes seems arbitrary.
    The essential thing in deciding which hypothesis is the null is that one must be able to
    calculate the distribution of the test statistic when the null hypothesis is true.
    Type I error: erroneously rejecting the null hypothesis.
    Type II error: erroneously failing to reject the null hypothesis.
    Significance level: the chance of a type I error.
    Power against a given alternative: the chance of correctly rejecting the null when that
    alternative is true.
    If we have a family of tests of a given hypothesis, indexed by significance level, so
    that we can test the null hypothesis at
    any significance level, the <em>P</em>-value of the null hypothesis given a set of
    data is the smallest significance level for which all tests in the family with
    significance levels larger than <em>P</em> would reject the null hypothesis.
    That is,
</p>

<p class="math">
	P = inf { &alpha; : tests at level &alpha;' would reject the null whenever &alpha;' &ge; &alpha;}.
</p>

<p>
    Note: the <em>P</em>-value <em>is not</em> the chance that the null hypothesis is true.
    The <em>P</em>-value is the probability of a particular event, calculated <em>assuming</em>
    that the null hypothesis is true.
    The null hypothesis is either true or false.
    We don't (usually) talk about the probability that hypotheses are true.
</p>


<h2>Parametric/non-parametric/robust</h2>

<p>
    Parametric statistics: assume a functional form for the probability distribution of
    the observations; worry perhaps about some parameters in that function.
    Non-parametric statistics: fewer, weaker assumptions about the probability distribution of
    the observations; perhaps assume that the observations
    are i.i.d. or exchangeable, or that the underlying probability distribution
    is continuous (has a density).
    (Modern nonparametrics includes <em>infinitely parametric</em> problems,
    such as density estimation and function estimation, where specifying the
    unknown requires possibly infinitely many parameters.)
    Robust: assume a functional form for the probability distribution, but
    worry about whether the procedure is sensitive to &quot;small&quot;
    departures from that assumed form.
    The sense in which the departure is
    assumed to be small, and the kinds of departures that are permitted,
    are up to the practitioner.
</p>

<p>
    Nonparametric methods based on randomization or ranks are about as robust as
    can be.
    They are our next topics.
</p>




</form>

<script language="JavaScript1.4" type="text/javascript" type="text/javascript"><!--
    writeMiscFooter(false);
// -->
</script>
</body>
</html>

