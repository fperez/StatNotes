{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57649ae7-96b1-45ed-b781-bf94e921580d",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "### Statistics 240, spring 2023\n",
    "\n",
    "There are two term projects involving open-source code contributions, a longer term project involving data analysis, and several shorter assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe930e-6146-40d5-ad6f-07a12d46b25c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Term project: Contributing to an open source package and joining the academic publishing ecosystem\n",
    "\n",
    "+ If you do not already have an ORCID, sign up for one at https://orcid.org/. \n",
    "\n",
    "+ See https://github.com/statlab/permute and https://statlab.github.io/permute/\n",
    "\n",
    "+ Read https://statlab.github.io/permute/dev/index.html\n",
    "\n",
    "+ Clone https://github.com/statlab/permute to your own device or to datahub. Set upstream to be a repository within your Berkeley Github repo for Stat 240.\n",
    "\n",
    "\n",
    "**Part 1.** Unit tests.\n",
    "\n",
    "+ Look at the code coverage for the latest build, https://app.codecov.io/gh/statlab/permute\n",
    "\n",
    "+ Identify at least one function that does not have complete test coverage.\n",
    "\n",
    "+ Fork your `permute` main branch to make a testing branch\n",
    "\n",
    "+ Write a unit test that exercises functionality that was not previously tested. Document your unit test\n",
    "using an appropriate docstring that follows PEP8 (https://peps.python.org/pep-0008/) and PEP257 (https://peps.python.org/pep-0257/)\n",
    "\n",
    "+ Verify that your test increases the coverage, using covecov. Github automation can be configured to\n",
    "run codecov whenever you push your branch and/or make a pull request. \n",
    "That's how it's set up at https://github.com/statlab/permute\n",
    "\n",
    "+ When everything is verified, make a pull request at https://github.com/statlab/permute to have your test included\n",
    "in the package. Congratulations! You've made a pull request for an open source project! If the moderator approves\n",
    "your pull request, you will be listed as a contributor.\n",
    "\n",
    "+ Bonus: find one or more existing unit tests in `permute` that do not exercise the code well and write better unit tests to replace them. Verify that you have not decreased the coverage. Make a pull request to the project once you have verified everything.\n",
    "\n",
    "**Part 2.** New functionality.\n",
    "\n",
    "There are many nonparametric tests and confidence procedures not yet in `permute`, for instance:\n",
    "\n",
    "+ tests and confidence intervals/sequences for bounded means using betting martingales, for a variety of betting strategies; ALPHA\n",
    "\n",
    "+ tests and confidence intervals for the treatment effect for binary treatment and binary outcomes\n",
    "\n",
    "+ tests and confidence sets for percentiles of the treatment effect for bounded outcomes\n",
    "\n",
    "+ exact tests and confidence intervals for the mean from stratified samples, including methods based on greedy discrete optimization and on supermartingales, $E$-values, and union-intersection tests.\n",
    "\n",
    "+ Gaffke's bound\n",
    "\n",
    "+ Romano's projection of the empirical approach to testing nonparametric hypotheses, including symmetry, exchangeability, and independence\n",
    "\n",
    "Code up one of those, or some other nonparametric method from the course that is not yet in `permute`.\n",
    "Make sure the calling signature of your function is parallel to that of similar functions in the package.\n",
    "Make sure the function has the right level of abstraction, and that you provide implementations of\n",
    "a number of sub-functions if that is warranted. \n",
    "For instance, for methods based on betting martingales,\n",
    "you should implement a number of betting strategies; for the ALPHA approach, you should implement\n",
    "a number of \"estimators.\" \n",
    "If the method can be computed exactly in some circumstances but needs to be approximated/simulated/randomized \n",
    "in others, provide both options.\n",
    "Document your function. Write unit tests that cover its functionality completely. \n",
    "When you're satisfied that it's correct, complete, tidy, and well documented, \n",
    "make a pull request to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79924d21-73e9-4d97-934d-9c3b31c06aaa",
   "metadata": {},
   "source": [
    "### Term project 2: Nonparametric data analysis\n",
    "\n",
    "Identify a paper that is interesting to you that analyzed data parametrically, but a nonparametric\n",
    "analysis is justified by the way the data were collected, e.g., by randomization. \n",
    "\n",
    "[MORE TO COME]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd56641-f0dd-489c-a6dc-2d3e25c032d9",
   "metadata": {},
   "source": [
    "### Assignment 1.\n",
    "Due 1/24/2023, 11:59pm.\n",
    "\n",
    "1. Identify the question and source of data for your term project.\n",
    "\n",
    "1. Let $A$, $B$ and $C$ be sets.\n",
    "Show that $A \\cup (B \\cap C ) = (A \\cup B ) \\cap (A \\cup C)$, and\n",
    "$A \\cap (B \\cup C) = (A \\cap B ) \\cup (A \\cap C )$.\n",
    "\n",
    "1. Let $A$ and $B$ be sets.\n",
    "Show that $A-B = \\emptyset$ implies $A \\subset B$.\n",
    "\n",
    "1. Show that for any sets $A$, $B$, $C$, $D$,\n",
    "$(A \\bigotimes B) \\cap (C \\bigotimes D) = (A \\cap C) \\bigotimes (B \\cap D)$.\n",
    "\n",
    "1. Show that for any function $f$ with domain $\\mathcal{X}$, if $A, B \\subset \\mathcal{X}$,\n",
    "then $f(A \\cap B) = fA \\cap fB$, and that\n",
    "$f(A \\cup B ) = fA \\cup fB$.\n",
    "\n",
    "1. Let $f$ be a function with co-domain $\\mathcal{Y}$, and $A, B \\subset \\mathcal{Y}$.\n",
    "Does $f^{-1} (A \\cap B) = f^{-1}A \\cap f^{-1}B$?\n",
    "Does $f^{-1} (A \\cup B ) = f^{-1}A \\cup f^{-1} B$? \n",
    "\n",
    "1. Let $f$ have domain $\\mathcal{X}$ and co-domain $\\mathcal{Y}$, and suppose that $A \\subset \\mathcal{X}$\n",
    "and $B \\subset \\mathcal{Y}$.\n",
    "Does $f^{-1}(f(A)) = A$?\n",
    "Does $f(f^{-1}B) = B$?\n",
    "\n",
    "1. Let $\\mathcal{G}$ be a group with identity $e$.\n",
    "Show that $ae = (a^{-1})^{-1} = a$. (That is, show that $e$ is not only the identity from the left, it\n",
    "is also the identity from the right, and that if $a^{-1}a = e$, then\n",
    "$aa^{-1} = e$.)\n",
    "\n",
    "1. Let $a, b, c, d \\in F$, where $F$ is a field.\n",
    "Show that if $b,d \\ne 0$, then $a/b+c/d = (ad+bc)/bd$.\n",
    "\n",
    "1. Show that $A= \\{0,1,2, \\cdots, p-1 \\}$ with $p$ prime is a field, if\n",
    "addition and multiplication are defined modulo $p$.\n",
    "What breaks down if $p$ is not prime?\n",
    "For $p=7$, show that the multiplicative inverse of 2 is 4.\n",
    "\n",
    "1. Suppose $\\{ M_i \\}_{i \\in \\mathbb{N}}$ is a countable collection of supermartingales with respect to the\n",
    "same filtration. Show that every positive linear combination of any finite subset of them is a supermartingale with\n",
    "respect to the same filtration, and that the expected value of the first term in that supermartingale is\n",
    "the same positive linear combination of the expected values of the first terms.\n",
    "\n",
    "1. In light of the previous result, a convex combination of a finite collection of nonnegative supermartingales (on the same filtration) starting at one is a nonnegative supermartingale starting at one. How can that be used to construct $E$-values?\n",
    "\n",
    "1. Is there an analogous result when the supermartingales are not defined with respect to a common filtration?\n",
    "How might you construct a common filtration from the \"marginal\" filtrations?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f583b9-275f-4c9f-8fd8-b1fef400d151",
   "metadata": {},
   "source": [
    "### Assignment 2.\n",
    "\n",
    "1. Write a brief (1--2 page) research proposal for your term project.  The proposal\n",
    "should include the scientific question you intend to address, the source of\n",
    "the data, and the primary reference you plan to use.\n",
    "If you need help picking a topic, see me.\n",
    "\n",
    "1. Explain the ``randomization model'' and some of its advantages and limitations.\n",
    "State the strong null hypothesis for the randomization model and the typical weak null.\n",
    "\n",
    "1. For the randomization model, state some pros and cons of the Wilcoxon \n",
    "rank sum test versus a permutation test based on the sample mean or on the permutation distribution\n",
    "of the $t$ statistic, compared\n",
    "with the parametric two-sample Student $t$-test.\n",
    "State the null hypothesis for each of the tests.\n",
    "\n",
    "1. Consider an experiment involving 9 subjects, 5 assigned at random to treatment\n",
    "and 4 to control.\n",
    "We want to test the null hypothesis that \"treatment makes no difference\"\n",
    "at significance level 10%.\n",
    "For each individual, we measure a quantitative response.\n",
    "Think about the omnibus alternative, about the one-sided shift alternative\n",
    "that treatment increases the response, and about the two-sided shift\n",
    "alternative that treatment increases or decreases the response.\n",
    "Consider four tests: the Wilcoxon rank-sum test (using mid-ranks for ties), a\n",
    "permutation test based on the difference in the sample means for the control and\n",
    "the treatment groups, the Smirnov test, and a 2-sample $t$-test based on the\n",
    "difference in the sample means for the control and treatment groups.\n",
    "\n",
    "    1. Explain the assumptions of each test, including a precise statement of the null hypotheses.\n",
    "\n",
    "    1. State strong and weak versions of each null hypothesis. Note whether the nominal significance level of each test is for the strong or weak null hypothesis.\n",
    "\n",
    "    1. Find or estimate by simulation the power of each test against the one-sided shift alternative that treatment increases each individual's response by 1 unit. If the power depends on additional unspecified features of the treatment effect or on features of the baseline responses of the subjects, explain what those features are, and find the power for a few different values of those features.\n",
    "\n",
    "    1. For one-sided versions of the Wilcoxon rank-sum test, the permutation test using the sample mean, and the $t$-test, and for the Smirnov test, find the $P$-values for the following hypothetical data, and the power against a shift alternative that treatment increases the response by one unit:\n",
    "| treatment | 1 | 2 | 3 | 3 | 4 |\n",
    "|----------|---:|----:|----:|----:|----:|\n",
    "| control   | 0 | 1.5 | 2.5 | 3.5 |\n",
    "    E.  Which test do you think is best in this situation, absent any additional information about the nature of the experiment? Why?\n",
    "\n",
    "5. Consider the Smirnov test for an experiment involving 5 subjects, 2 assigned at random to treatment and 3 to control.\n",
    "\n",
    "    1. Enumerate all possible values of the test statistic and their probabilities under the strong null hypothesis of no treatment effect, assuming no ties among the data.\n",
    "    \n",
    "    1. Now estimate the probabilities by simulation, using 10,000 replications.\n",
    "\n",
    "    1. Calculate the true (theoretical) standard error of the simulated probabilities.\n",
    "\n",
    "    1. What is the joint distribution of the number of times the test statistic takes each of its possible values?\n",
    "\n",
    "    1. Sketch how you would test the hypothesis that the true probabilities are equal to the values you calculated in the first part of this question using the empirical frequencies you observed in the second part.\n",
    "\n",
    "    1. What software package are you using to do the calculations? What is its default algorithm for calculating pseudo-random numbers? What is the period of that pseudo-random number generator? What is the largest number of objects for which that generator can give you all permutations? Is there an option in the package to use a better pseudo-random number generator?  If so, which one?\n",
    "    \n",
    "6. Give statistical interpretations and theoretical justifications for using $\\mbox{hits}/\\mbox{reps}$ and $(\\mbox{hits}+1)/(\\mbox{reps}+1)$ as the simulation $P$-value. Which do you prefer? Why?\n",
    "\n",
    "7. Give five real-world examples of a scientific null hypothesis that can be expressed as the invariance of\n",
    "a probability model for the data under the action of some group. In each case, identify the \"scientific\" invariance and the corresponding group invariance for the data. Explain how you could use that invariance to test the scientific\n",
    "hypotheses without knowing anything else about the probability model for the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c1c91-46ad-44be-b8b6-5d03bf9c1d18",
   "metadata": {},
   "source": [
    "### Assignment 3.\n",
    "\n",
    "1. Show that the collection of all sets of the form $(-\\infty, x] \\times (-\\infty, y]$\n",
    "comprise a Vapnik-Cervonenkis class (V-C class) over the plane.\n",
    "\n",
    "1. Show that intersections and finite unions of V-C classes are V-C classes. Show that countable unions of V-C classes need not be V-C classes.\n",
    "\n",
    "1. The file https://statistics.berkeley.edu/~stark/Java/Data/lomaPrieta.dat contains 221 observations of the times of putative aftershocks of the 17 October 1989 earthquake in Loma Prieta, California.\n",
    "There are 222 lines in the file.\n",
    "The first is 0, the main shock, which occurred at 4:15:43pm.\n",
    "The other lines are the times in days from the main event to the aftershocks,\n",
    "defined as earthquakes determined to have magnitude 3.0 and above, focal depth\n",
    "of 0--20km, and epicenter within 40km of the epicenter of the Loma Prieta earthquake.\n",
    "The data are from the UC Berkeley Seismographic Stations, courtesy of\n",
    "Dr. Bob Uhrhammer.\n",
    "(Hint: see See pp. 109--116 and Labs 10 and 11 in Freedman (2005), _Statistical Models: Theory and Practice_.)\n",
    "                \n",
    "    1. According to one theory, aftershocks follow the modified Omori Law. If so, the data are an iid sample of size 221, sorted into increasing order, from the density $C/(a+t)^b$, where $C$ is a normalizing constant, $a$ and $b$ are parameters,  and $t$ is time in the interval 0 to 805 days.\n",
    "                                \n",
    "        1. There are natural restrictions on $a$ and $b$ for the density to peak at 0 and decrease monotonically (Aftershocks generally are most frequent immediately after the main shock, then decrease in frequency.) What are those restrictions?\n",
    "        \n",
    "        1. Find the MLE for $a$ and $b$ from the Loma Prieta data.\n",
    "        \n",
    "        1. Compute the observed information matrix of the parameter estimates.\n",
    "        \n",
    "        1. Is the MLE biased in this application? Is the observed information matrix a good approximation to the variance-covariance matrix of $a$ and $b$?\n",
    "    \n",
    "    1. Estimate the probability density of aftershocks of the Loma Prieta earthquake for the time interval 0 to 805 days, conditional on the event that there  were 221 aftershocks during that period. (Assume that, conditional on the total number of aftershocks in the interval, the times of the aftershocks are iid random variables with common density $f$, whose functional form is unknown; the data are those times sorted into increasing order.) Use the estimators listed below. Compare and contrast the estimates. You might want to plot the density estimates on a semi-logarithmic scale (linear in time, but logarithmic in the density: the rate of aftershocks is extremely high in the first few days). Please implement the algorithms yourself---do not use \"canned\" routines from a package.  You will get a better feel for the methods if you have to code them. But feel free to use a canned package to check your results.\n",
    "                                \n",
    "        1. Histogram (use several choices of bin width).\n",
    "                                        \n",
    "        1. The naive estimator (use several bin widths).\n",
    "                                        \n",
    "        1. A kernel density estimate (use several bandwidths). The kernel you use is up to you, but say why you chose it.\n",
    "        1. The nearest neighbor estimate (use several neighborhood sizes).\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81207657-29d0-4481-b1d7-d6bb064b39f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
